{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd48274a",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Machines (SVM)\n",
    "\n",
    "This notebook presents an in-depth guide to understanding Support Vector Machines (SVMs), a powerful machine learning algorithm widely used for classification and, in some cases, regression. SVM is known for its robustness and effectiveness, especially in cases where data is not perfectly linearly separable.\n",
    "\n",
    "## Objectives\n",
    "1. **Comprehensive Theory** - To understand the mathematical and conceptual framework of SVM.\n",
    "2. **Detailed Implementation** - A step-by-step code implementation for training, tuning, and evaluating an SVM model.\n",
    "3. **Practical Insights** - Key considerations when working with SVM, including kernel selection, hyperparameter tuning, and evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420c274",
   "metadata": {},
   "source": [
    "\n",
    "## Theory and Working of SVM\n",
    "\n",
    "### 1. Hyperplanes and Linear Separability\n",
    "A **hyperplane** is the decision boundary that SVM uses to classify data points in a feature space. In two dimensions, it appears as a line, while in higher dimensions, it extends to planes or hyperplanes. SVM seeks to find the optimal hyperplane that distinctly separates classes of data points.\n",
    "\n",
    "In cases where the data is linearly separable, SVM can directly find this boundary; however, when the data is not linearly separable, SVM uses the kernel trick to transform the data.\n",
    "\n",
    "### 2. Support Vectors\n",
    "Support vectors are the critical data points nearest to the hyperplane. These vectors determine the margin of the classifier; moving them changes the hyperplane's position. The SVM model is driven by these data points rather than all points, making it computationally efficient.\n",
    "\n",
    "### 3. Margins and Optimal Hyperplane\n",
    "The margin is the distance between the hyperplane and the nearest support vector. SVM aims to maximize this margin, leading to a model that is less likely to overfit.\n",
    "\n",
    "### 4. The Kernel Trick\n",
    "For data that is not linearly separable in its original form, SVM uses a mathematical technique called the **kernel trick**. This trick allows SVM to implicitly map data to a higher-dimensional space where a hyperplane can separate classes. Common kernels include:\n",
    "- **Linear**: Suitable for linearly separable data.\n",
    "- **Polynomial**: Useful for non-linear data with polynomial relationships.\n",
    "- **Radial Basis Function (RBF)**: Effective for highly complex relationships.\n",
    "\n",
    "### 5. Hyperparameters in SVM\n",
    "The main hyperparameters include:\n",
    "- **C (Regularization Parameter)**: Controls the trade-off between achieving a low error on training data and a simpler decision boundary with a wider margin.\n",
    "- **Kernel**: Specifies the kernel function, affecting how the model transforms and separates the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4aa1de",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load the dataset and perform any necessary preprocessing, including feature scaling, which is crucial for SVM as it is sensitive to the scale of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86beb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6be0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d54ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Features: \", cancer.feature_names)\n",
    "# print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cancer.data\n",
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train[:5], y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3809e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec985e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbaf5d",
   "metadata": {},
   "source": [
    "\n",
    "## Training the SVM Model\n",
    "\n",
    "Now, we initialize the SVM classifier and specify key hyperparameters such as `C` and `kernel`. In SVM:\n",
    "- **C** is a regularization parameter, where a smaller value of `C` creates a wider margin but may misclassify some data points. A larger `C` tries to correctly classify all training points but can lead to overfitting.\n",
    "- **Kernel** defines the type of transformation applied to the data for better separability.\n",
    "\n",
    "We then train the model on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1b202",
   "metadata": {},
   "source": [
    "\n",
    "## Model Evaluation\n",
    "\n",
    "After training, it's essential to evaluate the model on test data to understand its generalization performance. We can use various metrics, including:\n",
    "- **Accuracy**: Overall correctness of the model.\n",
    "- **Precision and Recall**: Metrics that help evaluate performance in cases of class imbalance.\n",
    "- **F1-Score**: Harmonic mean of precision and recall, giving a balanced view of the model's performance.\n",
    "\n",
    "These metrics will give us a comprehensive view of how well our SVM model performs on unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
